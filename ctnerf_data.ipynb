{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9d3729f-3229-4106-b577-efbc4a769734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM GAN_stability https://github.com/LMescheder/GAN_stability/blob/master/gan_training/config.py\n",
    "\n",
    "import yaml\n",
    "from torch import optim\n",
    "from os import path\n",
    "\n",
    "# General config\n",
    "def load_config(path, default_path):\n",
    "    ''' Loads config file.\n",
    "    Args:  \n",
    "        path (str): path to config file\n",
    "        default_path (bool): whether to use default path\n",
    "    '''\n",
    "    # Load configuration from file itself\n",
    "    with open(path, 'r') as f:\n",
    "        # cfg_special = yaml.load(f)\n",
    "        cfg_special = yaml.safe_load(f)\n",
    "\n",
    "    # Check if we should inherit from a config\n",
    "    inherit_from = cfg_special.get('inherit_from')\n",
    "\n",
    "    # If yes, load this config first as default\n",
    "    # If no, use the default_path\n",
    "    if inherit_from is not None:\n",
    "        cfg = load_config(inherit_from, default_path)\n",
    "    elif default_path is not None:\n",
    "        with open(default_path, 'r') as f:\n",
    "            # cfg = yaml.load(f)\n",
    "            cfg = yaml.safe_load(f)\n",
    "    else:\n",
    "        cfg = dict()\n",
    "\n",
    "    # Include main configuration\n",
    "    update_recursive(cfg, cfg_special)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def update_recursive(dict1, dict2):\n",
    "    ''' Update two config dictionaries recursively.\n",
    "    Args:\n",
    "        dict1 (dict): first dictionary to be updated\n",
    "        dict2 (dict): second dictionary which entries should be used\n",
    "    '''\n",
    "    for k, v in dict2.items():\n",
    "        # Add item if not yet in dict1\n",
    "        if k not in dict1:\n",
    "            dict1[k] = None\n",
    "        # Update\n",
    "        if isinstance(dict1[k], dict):\n",
    "            update_recursive(dict1[k], v)\n",
    "        else:\n",
    "            dict1[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9c9a53e-44e5-4180-94ab-34759b510988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "\n",
    "\n",
    "class ImageDataset(VisionDataset):\n",
    "    \"\"\"\n",
    "    Load images from multiple data directories.\n",
    "    Folder structure: data_dir/filename.png\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dirs, transforms=None):\n",
    "        # Use multiple root folders\n",
    "        if not isinstance(data_dirs, list):\n",
    "            data_dirs = [data_dirs]\n",
    "\n",
    "        # initialize base class\n",
    "        VisionDataset.__init__(self, root=data_dirs, transform=transforms)\n",
    "\n",
    "        self.filenames = []\n",
    "        root = []\n",
    "\n",
    "        for ddir in self.root:\n",
    "            filenames = self._get_files(ddir)\n",
    "            self.filenames.extend(filenames)\n",
    "            root.append(ddir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_files(root_dir):\n",
    "        return glob.glob(f'{root_dir}/*.png') + glob.glob(f'{root_dir}/*.jpg')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        img = Image.open(filename).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "class DRR(ImageDataset):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(DRR, self).__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "class Carla(ImageDataset):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Carla, self).__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "class CelebA(ImageDataset):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CelebA, self).__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "class CUB(ImageDataset):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CUB, self).__init__(*args, **kwargs)\n",
    "        \n",
    "\n",
    "class Cats(ImageDataset):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "      super(Cats, self).__init__(*args, **kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_files(root_dir):\n",
    "      return glob.glob(f'{root_dir}/CAT_*/*.jpg')\n",
    "\n",
    "\n",
    "class CelebAHQ(ImageDataset):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CelebAHQ, self).__init__(*args, **kwargs)\n",
    "    \n",
    "    def _get_files(self, root):\n",
    "        return glob.glob(f'{root}/*.npy')\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = np.load(self.filenames[idx]).squeeze(0).transpose(1,2,0)\n",
    "        if img.dtype == np.uint8:\n",
    "            pass\n",
    "        elif img.dtype == np.float32:\n",
    "            img = (img * 255).astype(np.uint8)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        img = Image.fromarray(img).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8c9be1f-595a-4896-96e6-d1e38a47215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import imageio\n",
    "import os\n",
    "import glob\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class  ImageFolder(Dataset):\n",
    "    \"\"\"docstring for ArtDataset\"\"\"\n",
    "    def __init__(self, root, transform=None):\n",
    "        super( ImageFolder, self).__init__()\n",
    "        self.root = root\n",
    "\n",
    "        self.frame = self._parse_frame()\n",
    "        self.transform = transform\n",
    "\n",
    "    def _parse_frame(self):\n",
    "        frame = []\n",
    "        folders = glob.glob(os.path.join(self.root, '*'))\n",
    "        for folder in folders:\n",
    "            for f in glob.glob(os.path.join(folder, '*.png')):\n",
    "                frame.append(f)\n",
    "        return frame\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.frame[idx]\n",
    "        img = Image.open(file).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "def InfiniteSampler(n):\n",
    "    \"\"\"Data sampler\"\"\"\n",
    "    i = n - 1\n",
    "    order = np.random.permutation(n)\n",
    "    while True:\n",
    "        yield order[i]\n",
    "        i += 1\n",
    "        if i >= n:\n",
    "            np.random.seed()\n",
    "            order = np.random.permutation(n)\n",
    "            i = 0\n",
    "\n",
    "class InfiniteSamplerWrapper(data.sampler.Sampler):\n",
    "    \"\"\"Data sampler wrapper\"\"\"\n",
    "    def __init__(self, data_source):\n",
    "        self.num_samples = len(data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(InfiniteSampler(self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2 ** 15\n",
    "\n",
    "def get_nsamples(data_loader, N):\n",
    "  x = []\n",
    "  n = 0\n",
    "  while n < N:\n",
    "    x_next = next(data_loader)\n",
    "    x_next = x_next.cuda(non_blocking=True)\n",
    "    x.append(x_next)\n",
    "    n += x_next.size(0)\n",
    "  x = torch.cat(x, dim=0)[:N]\n",
    "  return x\n",
    "\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "  model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "  return sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "\n",
    "def save_video(imgs, fname, as_gif=False, fps=24, quality=8):\n",
    "    # convert to np.uint8\n",
    "    imgs = (255 * np.clip(imgs.permute(0, 2, 3, 1).detach().cpu().numpy() / 2 + 0.5, 0, 1)).astype(np.uint8)\n",
    "    imageio.mimwrite(fname, imgs, fps=fps, quality=quality)\n",
    "    \n",
    "    if as_gif:  # save as gif, too\n",
    "        os.system(f'ffmpeg -i {fname} -r 15 '\n",
    "                  f'-vf \"scale=512:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" {os.path.splitext(fname)[0] + \".gif\"}')\n",
    "\n",
    "\n",
    "def color_depth_map(depths, scale=None):\n",
    "    \"\"\"\n",
    "    Color an input depth map.\n",
    "    Arguments:\n",
    "        depths -- HxW numpy array of depths\n",
    "        [scale=None] -- scaling the values (defaults to the maximum depth)\n",
    "    Returns:\n",
    "        colored_depths -- HxWx3 numpy array visualizing the depths\n",
    "    \"\"\"\n",
    "\n",
    "    _color_map_depths = np.array([\n",
    "      [0, 0, 0],  # 0.000\n",
    "      [0, 0, 255],  # 0.114\n",
    "      [255, 0, 0],  # 0.299\n",
    "      [255, 0, 255],  # 0.413\n",
    "      [0, 255, 0],  # 0.587\n",
    "      [0, 255, 255],  # 0.701\n",
    "      [255, 255, 0],  # 0.886\n",
    "      [255, 255, 255],  # 1.000\n",
    "      [255, 255, 255],  # 1.000\n",
    "    ]).astype(float)\n",
    "    _color_map_bincenters = np.array([\n",
    "      0.0,\n",
    "      0.114,\n",
    "      0.299,\n",
    "      0.413,\n",
    "      0.587,\n",
    "      0.701,\n",
    "      0.886,\n",
    "      1.000,\n",
    "      2.000,  # doesn't make a difference, just strictly higher than 1\n",
    "    ])\n",
    "  \n",
    "    if scale is None:\n",
    "      scale = depths.max()\n",
    "  \n",
    "    values = np.clip(depths.flatten() / scale, 0, 1)\n",
    "    # for each value, figure out where they fit in in the bincenters: what is the last bincenter smaller than this value?\n",
    "    lower_bin = ((values.reshape(-1, 1) >= _color_map_bincenters.reshape(1, -1)) * np.arange(0, 9)).max(axis=1)\n",
    "    lower_bin_value = _color_map_bincenters[lower_bin]\n",
    "    higher_bin_value = _color_map_bincenters[lower_bin + 1]\n",
    "    alphas = (values - lower_bin_value) / (higher_bin_value - lower_bin_value)\n",
    "    colors = _color_map_depths[lower_bin] * (1 - alphas).reshape(-1, 1) + _color_map_depths[\n",
    "      lower_bin + 1] * alphas.reshape(-1, 1)\n",
    "    return colors.reshape(depths.shape[0], depths.shape[1], 3).astype(np.uint8)\n",
    "\n",
    "\n",
    "# Virtual camera utils\n",
    "\n",
    "\n",
    "def to_sphere(u, v):\n",
    "    theta = 2 * np.pi * u\n",
    "    phi = np.arccos(1 - 2 * v)\n",
    "    cx = np.sin(phi) * np.cos(theta)\n",
    "    cy = np.sin(phi) * np.sin(theta)\n",
    "    cz = np.cos(phi)\n",
    "    s = np.stack([cx, cy, cz])\n",
    "    return s\n",
    "\n",
    "\n",
    "def polar_to_cartesian(r, theta, phi, deg=True):\n",
    "    if deg:\n",
    "        phi = phi * np.pi / 180\n",
    "        theta = theta * np.pi / 180\n",
    "    cx = np.sin(phi) * np.cos(theta)\n",
    "    cy = np.sin(phi) * np.sin(theta)\n",
    "    cz = np.cos(phi)\n",
    "    return r * np.stack([cx, cy, cz])\n",
    "\n",
    "\n",
    "def to_uv(loc):\n",
    "    # normalize to unit sphere\n",
    "    loc = loc / loc.norm(dim=1, keepdim=True)\n",
    "\n",
    "    cx, cy, cz = loc.t()\n",
    "    v = (1 - cz) / 2\n",
    "\n",
    "    phi = torch.acos(cz)\n",
    "    sin_phi = torch.sin(phi)\n",
    "\n",
    "    # ensure we do not divide by zero\n",
    "    eps = 1e-8\n",
    "    sin_phi[sin_phi.abs() < eps] = eps\n",
    "\n",
    "    theta = torch.acos(cx / sin_phi)\n",
    "\n",
    "    # check for sign of phi\n",
    "    cx_rec = sin_phi * torch.cos(theta)\n",
    "    if not np.isclose(cx.numpy(), cx_rec.numpy(), atol=1e-5).all():\n",
    "        sin_phi = -sin_phi\n",
    "\n",
    "    # check for sign of theta\n",
    "    cy_rec = sin_phi * torch.sin(theta)\n",
    "    if not np.isclose(cy.numpy(), cy_rec.numpy(), atol=1e-5).all():\n",
    "        theta = -theta\n",
    "\n",
    "    u = theta / (2 * np.pi)\n",
    "    assert np.isclose(to_sphere(u, v).detach().cpu().numpy(), loc.t().detach().cpu().numpy(), atol=1e-5).all()\n",
    "\n",
    "    return u, v\n",
    "\n",
    "\n",
    "def to_phi(u):\n",
    "    return 360 * u  # 2*pi*u*180/pi\n",
    "\n",
    "\n",
    "def to_theta(v):\n",
    "    return np.arccos(1 - 2 * v) * 180. / np.pi\n",
    "\n",
    "\n",
    "def sample_on_sphere(range_u=(0, 1), range_v=(0, 1)):\n",
    "    u = np.random.uniform(*range_u)\n",
    "    v = np.random.uniform(*range_v)\n",
    "    return to_sphere(u, v)\n",
    "\n",
    "\n",
    "def look_at(eye, at=np.array([0, 0, 0]), up=np.array([0, 0, 1]), eps=1e-5):\n",
    "    at = at.astype(float).reshape(1, 3)\n",
    "    up = up.astype(float).reshape(1, 3)\n",
    "\n",
    "    eye = eye.reshape(-1, 3)\n",
    "    up = up.repeat(eye.shape[0] // up.shape[0], axis=0)\n",
    "    eps = np.array([eps]).reshape(1, 1).repeat(up.shape[0], axis=0)\n",
    "\n",
    "    z_axis = eye - at\n",
    "    z_axis /= np.max(np.stack([np.linalg.norm(z_axis, axis=1, keepdims=True), eps]))\n",
    "\n",
    "    x_axis = np.cross(up, z_axis)\n",
    "    x_axis /= np.max(np.stack([np.linalg.norm(x_axis, axis=1, keepdims=True), eps]))\n",
    "\n",
    "    y_axis = np.cross(z_axis, x_axis)\n",
    "    y_axis /= np.max(np.stack([np.linalg.norm(y_axis, axis=1, keepdims=True), eps]))\n",
    "\n",
    "    r_mat = np.concatenate((x_axis.reshape(-1, 3, 1), y_axis.reshape(-1, 3, 1), z_axis.reshape(-1, 3, 1)), axis=2)\n",
    "\n",
    "    return r_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f4d0c21-fb89-48f5-906f-fad90bbe82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import *\n",
    "\n",
    "def save_config(outpath, config):\n",
    "    from yaml import safe_dump\n",
    "    with open(outpath, 'w') as f:\n",
    "        safe_dump(config, f)\n",
    "\n",
    "\n",
    "def update_config(config, unknown):\n",
    "    # update config given args\n",
    "    for idx,arg in enumerate(unknown):\n",
    "        if arg.startswith(\"--\"):\n",
    "            if (':') in arg:\n",
    "                k1,k2 = arg.replace(\"--\",\"\").split(':')\n",
    "                argtype = type(config[k1][k2])\n",
    "                if argtype == bool:\n",
    "                    v = unknown[idx+1].lower() == 'true'\n",
    "                else:\n",
    "                    if config[k1][k2] is not None:\n",
    "                        v = type(config[k1][k2])(unknown[idx+1])\n",
    "                    else:\n",
    "                        v = unknown[idx+1]\n",
    "                print(f'Changing {k1}:{k2} ---- {config[k1][k2]} to {v}')\n",
    "                config[k1][k2] = v\n",
    "            else:\n",
    "                k = arg.replace('--','')\n",
    "                v = unknown[idx+1]\n",
    "                argtype = type(config[k])\n",
    "                print(f'Changing {k} ---- {config[k]} to {v}')\n",
    "                config[k] = v\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_data(config):\n",
    "    H = W = imsize = config['data']['imsize']\n",
    "    dset_type = config['data']['type']\n",
    "    fov = config['data']['fov']\n",
    "\n",
    "    transforms = Compose([\n",
    "        Resize(imsize),\n",
    "        ToTensor(),\n",
    "        Lambda(lambda x: x * 2 - 1),\n",
    "    ])\n",
    "\n",
    "    kwargs = {\n",
    "        'data_dirs': config['data']['datadir'],\n",
    "        'transforms': transforms\n",
    "    }\n",
    "\n",
    "    if dset_type == 'carla':\n",
    "        dset = Carla(**kwargs)\n",
    "\n",
    "    elif dset_type == 'drr':\n",
    "        dset = DRR(**kwargs)\n",
    "\n",
    "    elif dset_type == 'celebA':\n",
    "        assert imsize <= 128, 'cropped GT data has lower resolution than imsize, consider using celebA_hq instead'\n",
    "        transforms.transforms.insert(0, RandomHorizontalFlip())\n",
    "        transforms.transforms.insert(0, CenterCrop(108))\n",
    "\n",
    "        dset = CelebA(**kwargs)\n",
    "\n",
    "    elif dset_type == 'celebA_hq':\n",
    "        transforms.transforms.insert(0, RandomHorizontalFlip())\n",
    "        transforms.transforms.insert(0, CenterCrop(650))\n",
    "\n",
    "        dset = CelebAHQ(**kwargs)\n",
    "\n",
    "    elif dset_type == 'cats':\n",
    "      transforms.transforms.insert(0, RandomHorizontalFlip())\n",
    "      dset = Cats(**kwargs)\n",
    "  \n",
    "    elif dset_type == 'cub':\n",
    "        dset = CUB(**kwargs)\n",
    "\n",
    "    dset.H = dset.W = imsize\n",
    "    dset.focal = W/2 * 1 / np.tan((.5 * fov * np.pi/180.))\n",
    "    radius = config['data']['radius']\n",
    "    render_radius = radius\n",
    "    if isinstance(radius, str):\n",
    "        radius = tuple(float(r) for r in radius.split(','))\n",
    "        render_radius = max(radius)\n",
    "    dset.radius = radius\n",
    "\n",
    "    # compute render poses\n",
    "    N = 40\n",
    "    theta = 0.5 * (to_theta(config['data']['vmin']) + to_theta(config['data']['vmax']))\n",
    "    angle_range = (to_phi(config['data']['umin']), to_phi(config['data']['umax']))\n",
    "    render_poses = get_render_poses(render_radius, angle_range=angle_range, theta=theta, N=N)\n",
    "\n",
    "    print('Loaded {}'.format(dset_type), imsize, len(dset), render_poses.shape, [H,W,dset.focal,dset.radius], config['data']['datadir'])\n",
    "    return dset, [H,W,dset.focal,dset.radius], render_poses\n",
    "\n",
    "\n",
    "def get_render_poses(radius, angle_range=(0, 360), theta=0, N=40, swap_angles=False):\n",
    "    poses = []\n",
    "    theta = max(0.1, theta)\n",
    "    for angle in np.linspace(angle_range[0],angle_range[1],N+1)[:-1]:\n",
    "        angle = max(0.1, angle)\n",
    "        if swap_angles:\n",
    "            loc = polar_to_cartesian(radius, theta, angle, deg=True)\n",
    "        else:\n",
    "            loc = polar_to_cartesian(radius, angle, theta, deg=True)\n",
    "        R = look_at(loc)[0]\n",
    "        RT = np.concatenate([R, loc.reshape(3, 1)], axis=1)\n",
    "        poses.append(RT)\n",
    "    return torch.from_numpy(np.stack(poses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6665a73a-4400-4200-830d-7655ef65e5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded drr 128 1440 torch.Size([40, 3, 4]) [128, 128, 731.523347376726, (9.5, 10.5)] /scratch/gpfs/agp3/nerf_from_nothing/data/chest_xrays\n",
      "FAR\n",
      "12.5\n",
      "NEAR\n",
      "7.5\n",
      "\n",
      "TRAIN_DATASET:\n",
      "-------------------------------------------------\n",
      "Dataset DRR\n",
      "    Number of datapoints: 1440\n",
      "    Root location: ['/scratch/gpfs/agp3/nerf_from_nothing/data/chest_xrays']\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=128, interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               Lambda()\n",
      "           )\n",
      "<class '__main__.DRR'>\n",
      "\n",
      "HWFR:\n",
      "-------------------------------------------------\n",
      "[128, 128, 731.523347376726, (9.5, 10.5)]\n",
      "\n",
      "RENDER_POSES.SHAPE:\n",
      "-------------------------------------------------\n",
      "torch.Size([40, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "import sys\n",
    "\n",
    "# Arguments\n",
    "config = load_config(\"configs/chest.yaml\", 'configs/default.yaml')\n",
    "\n",
    "config['data']['fov'] = float(config['data']['fov'])\n",
    "config = update_config(config, [])\n",
    "\n",
    "out_dir = os.path.join(config['training']['outdir'], config['expname'])\n",
    "checkpoint_dir = path.join(out_dir, 'chkpts')\n",
    "\n",
    "# Create missing directories\n",
    "if not path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "if not path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Save config file\n",
    "save_config(os.path.join(out_dir, 'config.yaml'), config)\n",
    "\n",
    "# Dataset\n",
    "train_dataset, hwfr, render_poses = get_data(config)\n",
    "# in case of orthographic projection replace focal length by far-near\n",
    "print(\"FAR\")\n",
    "print(config['data']['far'])\n",
    "print(\"NEAR\")\n",
    "print(config['data']['near'])\n",
    "if config['data']['orthographic']:\n",
    "    hw_ortho = (config['data']['far']-config['data']['near'], config['data']['far']-config['data']['near'])\n",
    "    hwfr[2] = hw_ortho\n",
    "\n",
    "config['data']['hwfr'] = hwfr  # add for building generator\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"TRAIN_DATASET:\")\n",
    "print(\"-------------------------------------------------\")\n",
    "print(train_dataset)\n",
    "\n",
    "print(type(train_dataset))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"HWFR:\")\n",
    "print(\"-------------------------------------------------\")\n",
    "print(hwfr)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"RENDER_POSES.SHAPE:\")\n",
    "print(\"-------------------------------------------------\")\n",
    "print(render_poses.shape)\n",
    "\n",
    "# print(train_dataset, hwfr, render_poses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4285510e-2f5e-4d18-84c5-d8f45f95d919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "import yaml as y\n",
    "print(y.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f664949-5944-443a-a7fe-2f0221a56b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyyaml==5.4.1\n",
    "# DOWNGRADE YAML AS PER Stackoverflow\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "OR replace yaml.load() with yaml.safe_load(), which in the newer versions of yaml avoids the issue with positional argument count\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f44230b-1067-40b1-b9c8-61c8fd77521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA FOR NERF\n",
    "\n",
    "images = data['images']\n",
    "poses = data['poses']\n",
    "focal = hwfr[2]\n",
    "\n",
    "print(f'Images shape: {images.shape}')\n",
    "print(f'Poses shape: {poses.shape}')\n",
    "print(f'Focal length: {focal}')\n",
    "\n",
    "height = hwfr[0]\n",
    "width = hwfr[1]\n",
    "near, far = config['data']['near'], config['data']['far']\n",
    "\n",
    "n_training = 100\n",
    "testimg_idx = 101\n",
    "testimg, testpose = images[testimg_idx], poses[testimg_idx]\n",
    "\n",
    "plt.imshow(testimg)\n",
    "print('Pose')\n",
    "print(testpose)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
